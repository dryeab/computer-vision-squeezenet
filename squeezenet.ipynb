{"cells":[{"cell_type":"markdown","metadata":{},"source":["| Name        | ID |\n","|--------------|----|\n","| Tigist Wondimneh | UGR/2538/12 |\n","| Yared Tsegaye | UGR/8284/12 | \n","| Yeabsira Driba | UGR/4951/12 |"]},{"cell_type":"markdown","metadata":{},"source":["## Background Discussion"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-05-01T15:40:30.433702Z","iopub.status.busy":"2024-05-01T15:40:30.432654Z","iopub.status.idle":"2024-05-01T15:40:30.441274Z","shell.execute_reply":"2024-05-01T15:40:30.439938Z","shell.execute_reply.started":"2024-05-01T15:40:30.433660Z"}},"source":["### Overview of SqueezeNet\n","\n","**SqueezeNet** is a compact convolutional neural network (CNN) architecture that was introduced with the primary goal of reducing model size while retaining comparable accuracy to larger models like AlexNet. This architecture is particularly beneficial for deployment in environments where computational resources and storage are limited, such as mobile devices or embedded systems.\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Motivation and Design Principles\n","\n","The development of SqueezeNet was motivated by the need for more efficient neural network architectures that could operate within the constraints of low-power and low-memory devices. The key design principles behind SqueezeNet include:\n","\n","- **Parameter Reduction:** One of the main goals was to decrease the number of parameters drastically. This was achieved by employing smaller convolution filters and reducing the input channel depth using squeeze layers.\n","- **Maintaining Accuracy:** Despite the reduction in parameters, maintaining a high level of accuracy comparable to AlexNet was crucial. This was addressed through innovative architectural decisions like the use of Fire modules.\n","- **Incremental Refinement:** The architecture allows for flexible scaling of the model’s width (number of channels) and resolution of the input data, providing a means to balance between accuracy and model size.\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Key Components\n","\n","- **Fire Module:** At the heart of SqueezeNet is the Fire module, which consists of a squeeze layer (1x1 convolutions) followed by an expand layer that has a mix of 1x1 and 3x3 convolutions. This design significantly reduces the parameter count while allowing the network to expand its capacity to capture complex features.\n","- **Delayed Downsampling:** SqueezeNet strategically delays downsampling to deeper layers in the network, allowing for larger activation maps in the initial layers. This helps in maintaining high classification accuracy with fewer parameters.\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Comparisons with Predecessors\n","\n","- **AlexNet:** While AlexNet utilized large convolution filters in its first layer (11x11 filters), SqueezeNet uses much smaller filters throughout the network, leading to a drastic reduction in parameters. Both architectures aim for high accuracy on ImageNet, but SqueezeNet is much smaller in size.\n","- **Innovations Over AlexNet:** Unlike AlexNet, which uses a straightforward sequence of convolutional layers followed by fully connected layers, SqueezeNet introduces modular design through its Fire modules and eliminates most of the fully connected layers, which are parameter-heavy.\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-05-01T15:41:42.754682Z","iopub.status.busy":"2024-05-01T15:41:42.754270Z","iopub.status.idle":"2024-05-01T15:41:42.762099Z","shell.execute_reply":"2024-05-01T15:41:42.760773Z","shell.execute_reply.started":"2024-05-01T15:41:42.754655Z"}},"source":["The introduction of SqueezeNet marked a significant step forward in neural network design, pushing the boundaries of efficiency in deep learning architectures. It opened up new possibilities for AI applications in resource-constrained environments and influenced subsequent developments in network design aimed at reducing model size while maintaining performance.\n"]},{"cell_type":"markdown","metadata":{},"source":["# **SqueezeNet Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{},"source":["The below code implements the SqueezeNet architecture using TensorFlow and Keras, which is known for its efficiency by using Fire modules that combine squeezed and expanded convolution layers to reduce parameter count while maintaining performance. The model processes images with an initial size of 227×227×3227×227×3, includes several convolutional, pooling, and dropout layers, and concludes with a global average pooling and a softmax activation for classification into 1000 classes. This makes it ideal for resource-constrained environments needing high-performance image classification."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def fire_module(x, squeeze_planes, expand_planes):\n","    squeeze = tf.keras.layers.Conv2D(squeeze_planes, (1, 1), activation='relu', padding='same')(x)\n","    expand1 = tf.keras.layers.Conv2D(expand_planes, (1, 1), activation='relu', padding='same')(squeeze)\n","    expand3 = tf.keras.layers.Conv2D(expand_planes, (3, 3), activation='relu', padding='same')(squeeze)\n","    return tf.keras.layers.concatenate([expand1, expand3])\n","\n","def SqueezeNet(input_shape=(227, 227, 3), num_classes=1000):\n","    img_input = tf.keras.Input(shape=input_shape)\n","    \n","    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(img_input)\n","    x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n","    \n","    x = fire_module(x, squeeze_planes=16, expand_planes=64)\n","    x = fire_module(x, squeeze_planes=16, expand_planes=64)\n","    x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n","    \n","    x = fire_module(x, squeeze_planes=32, expand_planes=128)\n","    x = fire_module(x, squeeze_planes=32, expand_planes=128)\n","    x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n","    \n","    x = fire_module(x, squeeze_planes=48, expand_planes=192)\n","    x = fire_module(x, squeeze_planes=48, expand_planes=192)\n","    x = fire_module(x, squeeze_planes=64, expand_planes=256)\n","    x = fire_module(x, squeeze_planes=64, expand_planes=256)\n","    \n","    x = tf.keras.layers.Dropout(0.5)(x)\n","    x = tf.keras.layers.Conv2D(num_classes, (1, 1), padding='same', activation='relu')(x)\n","    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","    x = tf.keras.layers.Activation('softmax')(x)\n","    \n","    model = tf.keras.models.Model(inputs=img_input, outputs=x)\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["# **Data Preparation**"]},{"cell_type":"markdown","metadata":{},"source":["The following code includes functions for preprocessing images by scaling their pixel values to [-1, 1] and for loading and labeling images from a directory structure, resizing them to 227x227 for use with SqueezeNet. This setup is ideal for preparing image datasets for neural network training."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def preprocess_input_squeezenet(img):\n","    # Scale pixel values to the range [-1, 1]\n","    img /= 255.0\n","    img -= 0.5\n","    img *= 2.0\n","    return img\n","\n","def load_images_and_labels(base_path, image_size=(227, 227)):\n","    data = []\n","    labels = []\n","    classes = sorted(os.listdir(base_path))\n","\n","    for label, cls in enumerate(classes):\n","        cls_folder = os.path.join(base_path, cls)\n","        for img_name in os.listdir(cls_folder):\n","            img_path = os.path.join(cls_folder, img_name)\n","            img = tf.keras.preprocessing.image.load_img(img_path, target_size=image_size)\n","            img = tf.keras.preprocessing.image.img_to_array(img)\n","            img = preprocess_input_squeezenet(img)\n","            data.append(img)\n","            labels.append(label)\n","\n","    data = np.array(data, dtype=\"float32\")\n","    labels = np.array(labels)\n","    return data, labels, classes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train, y_train, classes = load_images_and_labels('./data/Alzheimer_s Dataset/train')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1765, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_test, y_test, classes = load_images_and_labels('./data/Alzheimer_s Dataset/test')"]},{"cell_type":"markdown","metadata":{},"source":["# **Model Training**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = SqueezeNet(input_shape=(227, 227, 3), num_classes=len(classes))  # len(classes) should be the number of classes in your dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Data augmentation generator\n","data_gen = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_loss, test_acc = model.evaluate(X_test, y_test)\n","print(f\"Test Accuracy: {test_acc*100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Convert the model to TensorFlow Lite format\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","# Save the TensorFlow Lite model to file\n","tflite_model_path = \"squeezenet_sign_language.tflite\"\n","with open(tflite_model_path, 'wb') as f:\n","    f.write(tflite_model)\n","print(f\"Model saved as TensorFlow Lite model at {tflite_model_path}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3251584,"sourceId":5657486,"sourceType":"datasetVersion"},{"datasetId":4919346,"sourceId":8282938,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
